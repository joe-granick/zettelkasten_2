---
aliases: []
---
Status: #idea
Tags: 

>Direct quote
>Regardless of which camp will prove to be right eventually, no one can deny that data is essential, for now. Both the research and industry trends in the recent decades show the success of ML relies more and more on the quality and quantity of data. Models are getting bigger and using more data. Back in 2013, people were getting excited when the One Billion Word Benchmark for Language Modeling was released, which contains 0.8 billion tokens.[23](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ch02.html#ch01fn56) Six years later, OpenAI’s GPT-2 used a dataset of 10 billion tokens. And another year later, GPT-3 used 500 billion tokens. The growth rate of the sizes of datasets is shown in [Figure 2-8](https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ch02.html#the_size_of_the_datasets_left_parenthes).
>![[language_model_size_and_performacne.png]]
>Figure 2-8. The size of the datasets (log scale) used for language models over time[^1]

Despite the strength of more data it will does not make up for poor quality data [^2]
[^1]:[[Huyen-Designing Machine Learning Systems]] ch 2
[^2]:[[202206161537-more data won't make up for low data quality]]