[[Claude Shannon]] created information theory for signal processing, mathematically defining information as the amount of uncertainty (termed [[Shannon entropy|information entropy]]) reduced when a signal is received ^[ [[How to Measure Anything]] pg 32]

Receiver has some prior state of uncertainty and new information removes some of that uncertainty, but not necessarily all of it. ^[ [[How to Measure Anything]] pg 32]

This is used to calculate ^[ [[How to Measure Anything]] pg 32]
	- limit of amount of information that can transmitted by a signal
	- minimum signal needed to correct for noise
	- max data compression possible
