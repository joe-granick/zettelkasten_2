>One of the key questions was whether these architectural patterns would converge. A year later, that doesn’t seem to have taken place.
>
>In particular, the analytic and operational ecosystems both continue to thrive. Cloud data warehouses like Snowflake have grown rapidly, focused largely on SQL users and business intelligence use cases. But adoption of other technologies has also accelerated — data lakehouses like Databricks, for instance, are adding customers faster than ever. Many data teams we spoke with confirmed that heterogeneity is likely here to stay in the data stack.
>
>Other core data systems — namely, ingestion and transformation — have proven similarly durable. This is especially visible in the modern business intelligence pattern, where the combination of Fivetran and dbt (or similar technologies) has become nearly ubiquitous. But it’s also true to an extent in operational systems, where _de facto_ standards like Databricks/Spark, Confluent/Kafka, and Astronomer/Airflow have emerged.
>
>
>There is growing recognition and clarity for the **lakehouse** architecture. We’ve seen this approach supported by a wide range of vendors (including AWS, Databricks, Google Cloud, Starburst, and Dremio) and data warehouse pioneers. The fundamental value of the lakehouse is to pair a robust storage layer with an array of powerful data processing engines like Spark, Presto, Druid/Clickhouse, Python libraries, etc.
> https://future.a16z.com/emerging-architectures-modern-data-infrastructure/
> [[Data Lakehouse]]
>


>Two parallel ecosystems have grown up around these broad use cases. The data warehouse forms the foundation of the analytics ecosystem. Most data warehouses store data in a structured format and are designed to quickly and easily generate insights from core business metrics, usually with SQL (although Python is growing in popularity). The data lake is the backbone of the operational ecosystem. By storing data in raw form, it delivers the flexibility, scale, and performance required for bespoke applications and more advanced data processing needs. Data lakes operate on a wide range of languages including Java/Scala, Python, R, and SQL.
>
>Each of these technologies has religious adherents, and building around one or the other turns out to have a significant impact on the rest of the stack (more on this later). But what’s really interesting is that modern data warehouses and data lakes are starting to resemble one another – both offering commodity storage, native horizontal scaling, semi-structured data types, ACID transactions, interactive SQL queries, and so on.
>
>The key question going forward: are data warehouses and data lakes are on a path toward convergence? That is, are they becoming interchangeable in the stack? Some experts believe this is taking place and driving simplification of the technology and vendor landscape. Others believe parallel ecosystems will persist due to differences in languages, use cases, or other factors.
>[[Data Lake]] ,[[Data Warehouse]]
>https://future.a16z.com/emerging-architectures-for-modern-data-infrastructure-2020/

>![[Pasted image 20220429081854.png]]
>[6:44]
>
>
>
>https://www.youtube.com/watch?v=MnP86QiWA3o&t=12s



>## What is a lakehouse?
>
New systems are beginning to emerge that address the limitations of data lakes. A lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. Lakehouses are enabled by a new system design: implementing similar data structures and data management features to those in a data warehouse directly on top of low cost cloud storage in open formats. They are what you would get if you had to redesign data warehouses in the modern world, now that cheap and highly reliable storage (in the form of object stores) are available.
>
**A lakehouse has the following key features:**
>
>-   **Transaction support:** In an enterprise lakehouse many data pipelines will often be reading and writing data concurrently. Support for ACID transactions ensures consistency as multiple parties concurrently read or write data, typically using SQL.
>-   **Schema enforcement and governance:** The Lakehouse should have a way to support schema enforcement and evolution, supporting DW schema architectures such as star/snowflake-schemas. The system should be able to [reason about data integrity](https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html), and it should have robust governance and auditing mechanisms.
>-   **BI support:** Lakehouses enable using BI tools directly on the source data. This reduces staleness and improves recency, reduces latency, and lowers the cost of having to operationalize two copies of the data in both a data lake and a warehouse.
>-   **Storage is decoupled from compute:** In practice this means storage and compute use separate clusters, thus these systems are able to scale to many more concurrent users and larger data sizes. Some modern data warehouses also have this property.
>-   **Openness:** The storage formats they use are open and standardized, such as Parquet, and they provide an API so a variety of tools and engines, including machine learning and Python/R libraries, can efficiently access the data **directly**.
>-   **Support for diverse data types ranging from unstructured to structured data**: The lakehouse can be used to store, refine, analyze, and access data types needed for many new data applications, including images, video, audio, semi-structured data, and text.
>-   **Support for diverse workloads:** including data science, machine learning, and SQL and analytics. Multiple tools might be needed to support all these workloads but they all rely on the same data repository.
>-   **End-to-end streaming:** Real-time reports are the norm in many enterprises. Support for streaming eliminates the need for separate systems dedicated to serving real-time data applications.
>
>These are the key attributes of lakehouses. Enterprise grade systems require additional features. Tools for security and access control are basic requirements. Data governance capabilities including auditing, retention, and lineage have become essential particularly in light of recent privacy regulations. Tools that enable data discovery such as data catalogs and data usage metrics are also needed. With a lakehouse, such enterprise features only need to be implemented, tested, and administered for a single system.
>https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html


### What is a data lakehouse?
>In the 80s data warehouses (DW) built on relational database management systems (RDBMS) became the norm for enterprise level business intelligence (BI). DWs imposed a structure on the data that made it easy to query and perform analytics on.
>As volumes of data grew, and methods for extracting insight through Artificial Intelligence  and Machine Learning, the limitations of the traditional DW became glaring. The strict structural requiremnts imposed on data warehouses meant only tabular data could be used. While adequate for traditional BI and reporting, this prevented the possibiltiy of extracting data from unstructured sources like text and video, from which AI/ML methods could now extract unprecedented insights. [[Inmon, Levin 2021]]
>
>Data lakes were developed in repsonse to this predicament.  Data lakes provided a location where all data could be placed regardless of structure, stored ina low cost format like Parquet and ORC. These formats were directly accessible for analytics and AI/ML. Unfortunately the lack of transactional guarentees, quality enforcemnt and governance typically resulted in the data lakes turning into disorganized messes where it was a challenge to actually get the data needed. [[Inmon, Levin 2021]]
>
>Data lakehouses were developed to overcome the limitations of data lakes and data warehouse by merging their respective strengths. Data combine data lakes and data warehouses by using the flexibility of storage provided by data lakes and adding strcture and data management capabilities similiar to data warehouses.
>-   **Transaction support:**  ACID transactions for consistency in environments with concurrent read and write operations.
>
>-   **Schema enforcement and governance:** Lakehouses support traditional data warehouse schemas.
>
>-   **BI support:** Allows BI tools to use source data directly reducing latency, improving recency, and saving cost as a separate copy of the data is no longer necessary for both a data lake and a data warehouse.
>
>-   **Decoupled storage and compute:** Storag and compute use separte cluster so the system can scale to more user and larger data sets.
>
>-   **Openness standard formats:** Enables a variety of tools across the spectrum of BI and AI/ML to directly access the data efficiently.
>
>-   **Support for many structures of data**: Lakehouses supports many forms of data including text, video, semi-structured, and formally structured.
>
>-   **Support for diverse workloads:** Supports workloads across BI, Analytics, AI/ML allowing tools native to these domains draw from the source.
>
>-   **End-to-end streaming:**  No need for a separate system to suppor streaming data for real time applications and reporting.
>[[Lorica et. al 2020]]

### What are the benefits of a data lakehouse?
>Data lakehouses overcome many of the limitations of data warehouses and data lakes. The lakehouse is uniquely able to to manage all of an enetrprises data no matter the format or source. It serve as a location to combine these disparate source, perform advanced analytics with ML like data lakes as well as the traditional reporting and BI workloads that were previously the domain of the DW. Some of the benefits of using a data lakehouse include:
>- Allows direct access using open file formats
>- Open API allows direct access to the dat without the use of proprietary engines avoiding vendor lock-in
>- Supports commonly used Analytics and AI/ML languages including, but not limited to SQL, Python, and R
>- Supports all types of structured and unstruictured data used in machine learning workflows
>- Supports dataframes natively, which are the primary data abstraction for many ML libraires including Tensorflow and Pytorch
>- Supports data versioning for ML experiments
>- Supports schema governance and enforcement with ACID transaction guarentess for structured data workflows used by traditional BI worflows similair to DW
>- Provides scalable low cost object storage like a Data Lakes.  (S3, Blob etc.)
>- [[Lorica et. al 2020]]


### What are the key vendors involved in this space?
> Major players in the data lakehouse space include:
>- Databrick
>- Apache Hudi
>- AWS
>- Google Cloud
>- Starburst
>- Dremio
>[[Bornsetin, Li, Casado 2022]]

### How do data lakehouses benefit your own data analytics
>Data lakehouses provide a central location for all analytics data. Analysts can access the data in the same way using the same BI tools and SQL queries that they are used to using with DWs. Since all the data can be stored in the lakehouse, analysts can now have access to all the raw data rather than what they would normally be limited to in the rigid DW structure. They can now explore the data and build pipelines themselves if they identify something useful. They can also mroe easily use ML models developed by data science and ML engineering teams on any data available.
>[[Armbrust et. al 2021]]